# To support both python 2 and python 3
from __future__ import division, print_function, unicode_literals

# Common imports
import numpy as np
import os
# TF1 Compatibility
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# To make this notebook's output stable across runs
np.random.seed(45)
tf.set_random_seed(45)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Ignore useless warnings (see SciPy issue #5998)
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")


import numpy as np
# Variable and Function Declaration
x_mean = 0
x_std = 0.55
y_mean = 0
y_std = 0.03
n = 1000
x = np.random.normal(x_mean, x_std, n)
yr = np.random.normal(y_mean, y_std, n)
y = 0.1 * x + 0.3 + yr


# Plotting The Function
plt.plot(x, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-2, 2, 0, 0.6])
plt.show()


# Regression Preparation
X_new = np.array([[-2], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]  # Adding x0 = 1 to each instance

# TF Variable Placeholder
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Gradient and Bias Randomly Declared
W = tf.Variable(np.random.randn(), name = "W") 
b = tf.Variable(np.random.randn(), name = "b") 

# TF Settings
learning_rate = 0.5
training_epochs = 5

# Hypothesis 
y_pred = tf.add(tf.multiply(X, W), b) 
  
# Mean Squared Error Cost Function 
cost = tf.reduce_sum(tf.pow(y_pred-Y, 2)) / (2 * n) 
  
# Gradient Descent Optimizer 
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) 
  
# Global Variables Initializer 
init = tf.global_variables_initializer() 


# Starting the Tensorflow Session 
with tf.Session() as sess: 
      
    # Initializing the Variables 
    sess.run(init) 
    
    # Plotting the original function graph
    plt.plot(x, y, "b.")
      
    # Iterating through all the epochs 
    for epoch in range(training_epochs): 
          
        # Feeding each data point into the optimizer using Feed Dictionary 
        for (_x, _y) in zip(x, y): 
            sess.run(optimizer, feed_dict = {X : _x, Y : _y}) 
          
        # Displaying the result after every epoch
        print("Epoch", (epoch + 1), ": ", "W =", sess.run(W), "; b =", sess.run(b)) 
        
        # Plotting the prediction graph
        y_pred = sess.run(W) * X_new_b + sess.run(b)
        style = "r--" if (epoch+1) < training_epochs else "b-"
        plt.plot(X_new_b, y_pred, style)
